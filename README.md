# ğŸ¯ Project Overview

ğŸ¤— This repository is a fork of lerobot by XenseRobotics, used for Xense's multimodal tactile data acquisition system. It implements numerous new features compared to the original repository.

## ğŸ”§ Installation

This repository is tested on Ubuntu 22.04, NVIDIA Unix Driver Archive >= 570.144. We strongly recommend using [`Mamba`](https://github.com/conda-forge/miniforge?tab=readme-ov-file#install) to manage your conda environments. You can also use conda, but it will take significantly longer. Due to the xensesdk dependencies, lerobot-xense works only with python 3.10 and pytorch 2.7.1+ with cuda-12.8 right now.

### ğŸ“¦ Environment Setup

**Step 1:** ğŸ“¥ Download and install the XenseVR PC Service `.deb` package from [XenseVR-PC-Service v0.1.0 Release](https://github.com/Vertax42/XenseVR-PC-Service/releases/tag/v0.1.0).

> âš ï¸ **Note:** This package has only been tested on **x86_64 Ubuntu 22.04**. Other architectures or distributions may not work right now.

```bash
# After downloading the .deb file, install it with:
sudo dpkg -i xensevr-pc-service_*.deb
# or simply run:
sudo apt-get install ./xensevr-pc-service_*.deb
```

**Step 2:** ğŸ“‚ Clone the repository and navigate into the directory:

```bash
git clone https://github.com/Vertax42/lerobot-xense.git
cd lerobot-xense
```

**Step 3:** ğŸ Create and activate the conda/mamba environment, then install dependencies:

```bash
bash ./setup_env.sh --mamba <optional_env_name>
mamba activate <optional_env_name> # or conda activate <optional_env_name>
bash ./setup_env.sh --install # you need to enter password for sudo access to install the dependencies
```

**Step 4:** ğŸ“Œ Check if FFmpeg 7.X is installed in your environment and `libsvtav1` encoder is supported:

```bash
mamba list | grep ffmpeg
ffmpeg -encoders | grep libsvtav1
```

## ğŸ“Š Weights & Biases

To use Weights and Biases for experiment tracking, log in with:

```bash
wandb login
```

**Note:** You will also need to enable WandB in the configuration. See below.

## ğŸ‘€ Visualize datasets

Check out [example 1](https://github.com/huggingface/lerobot/blob/main/examples/dataset/load_lerobot_dataset.py) that illustrates how to use our dataset class which automatically downloads data from the Hugging Face hub.

You can also locally visualize episodes from a dataset on the hub by executing our script from the command line:

```bash
lerobot-dataset-viz \
    --repo-id lerobot/pusht \
    --episode-index 0
```

or from a dataset in a local folder with the `root` option and the `--mode local` (in the following case the dataset will be searched for in `./my_local_data_dir/lerobot/pusht`)

```bash
lerobot-dataset-viz \
    --repo-id lerobot/pusht \
    --root ./my_local_data_dir \
    --mode local \
    --episode-index 0
```

Our script can also visualize datasets stored on a distant server. See `lerobot-dataset-viz --help` for more instructions.

## ğŸ”‘ The `LeRobotDataset` format

A dataset in `LeRobotDataset` format is very simple to use. It can be loaded from a repository on the Hugging Face hub or a local folder simply with e.g. `dataset = LeRobotDataset("lerobot/aloha_static_coffee")` and can be indexed into like any Hugging Face and PyTorch dataset. For instance `dataset[0]` will retrieve a single temporal frame from the dataset containing observation(s) and an action as PyTorch tensors ready to be fed to a model.

A specificity of `LeRobotDataset` is that, rather than retrieving a single frame by its index, we can retrieve several frames based on their temporal relationship with the indexed frame, by setting `delta_timestamps` to a list of relative times with respect to the indexed frame. For example, with `delta_timestamps = {"observation.image": [-1, -0.5, -0.2, 0]}` one can retrieve, for a given index, 4 frames: 3 "previous" frames 1 second, 0.5 seconds, and 0.2 seconds before the indexed frame, and the indexed frame itself (corresponding to the 0 entry). See example [1_load_lerobot_dataset.py](https://github.com/huggingface/lerobot/blob/main/examples/dataset/load_lerobot_dataset.py) for more details on `delta_timestamps`.

Under the hood, the `LeRobotDataset` format makes use of several ways to serialize data which can be useful to understand if you plan to work more closely with this format. We tried to make a flexible yet simple dataset format that would cover most type of features and specificities present in reinforcement learning and robotics, in simulation and in real-world, with a focus on cameras and robot states but easily extended to other types of sensory inputs as long as they can be represented by a tensor.

Here are the important details and internal structure organization of a typical `LeRobotDataset` instantiated with `dataset = LeRobotDataset("lerobot/aloha_static_coffee")`. The exact features will change from dataset to dataset but not the main aspects:

```
dataset attributes:
  â”œ hf_dataset: a Hugging Face dataset (backed by Arrow/parquet). Typical features example:
  â”‚  â”œ observation.images.cam_high (VideoFrame):
  â”‚  â”‚   VideoFrame = {'path': path to a mp4 video, 'timestamp' (float32): timestamp in the video}
  â”‚  â”œ observation.state (list of float32): position of an arm joints (for instance)
  â”‚  ... (more observations)
  â”‚  â”œ action (list of float32): goal position of an arm joints (for instance)
  â”‚  â”œ episode_index (int64): index of the episode for this sample
  â”‚  â”œ frame_index (int64): index of the frame for this sample in the episode ; starts at 0 for each episode
  â”‚  â”œ timestamp (float32): timestamp in the episode
  â”‚  â”œ next.done (bool): indicates the end of an episode ; True for the last frame in each episode
  â”‚  â”” index (int64): general index in the whole dataset
  â”œ meta: a LeRobotDatasetMetadata object containing:
  â”‚  â”œ info: a dictionary of metadata on the dataset
  â”‚  â”‚  â”œ codebase_version (str): this is to keep track of the codebase version the dataset was created with
  â”‚  â”‚  â”œ fps (int): frame per second the dataset is recorded/synchronized to
  â”‚  â”‚  â”œ features (dict): all features contained in the dataset with their shapes and types
  â”‚  â”‚  â”œ total_episodes (int): total number of episodes in the dataset
  â”‚  â”‚  â”œ total_frames (int): total number of frames in the dataset
  â”‚  â”‚  â”œ robot_type (str): robot type used for recording
  â”‚  â”‚  â”œ data_path (str): formattable string for the parquet files
  â”‚  â”‚  â”” video_path (str): formattable string for the video files (if using videos)
  â”‚  â”œ episodes: a DataFrame containing episode metadata with columns:
  â”‚  â”‚  â”œ episode_index (int): index of the episode
  â”‚  â”‚  â”œ tasks (list): list of tasks for this episode
  â”‚  â”‚  â”œ length (int): number of frames in this episode
  â”‚  â”‚  â”œ dataset_from_index (int): start index of this episode in the dataset
  â”‚  â”‚  â”” dataset_to_index (int): end index of this episode in the dataset
  â”‚  â”œ stats: a dictionary of statistics (max, mean, min, std) for each feature in the dataset, for instance
  â”‚  â”‚  â”œ observation.images.front_cam: {'max': tensor with same number of dimensions (e.g. `(c, 1, 1)` for images, `(c,)` for states), etc.}
  â”‚  â”‚  â”” ...
  â”‚  â”” tasks: a DataFrame containing task information with task names as index and task_index as values
  â”œ root (Path): local directory where the dataset is stored
  â”œ image_transforms (Callable): optional image transformations to apply to visual modalities
  â”” delta_timestamps (dict): optional delta timestamps for temporal queries
```

A `LeRobotDataset` is serialised using several widespread file formats for each of its parts, namely:

- hf_dataset stored using Hugging Face datasets library serialization to parquet
- videos are stored in mp4 format to save space
- metadata are stored in plain json/jsonl files

Dataset can be uploaded/downloaded from the HuggingFace hub seamlessly. To work on a local dataset, you can specify its location with the `root` argument if it's not in the default `~/.cache/huggingface/lerobot` location.

## Acknowledgment

- The LeRobot team ğŸ¤— for building SmolVLA [Paper](https://arxiv.org/abs/2506.01844), [Blog](https://huggingface.co/blog/smolvla).
- Thanks to Tony Zhao, Zipeng Fu and colleagues for open sourcing ACT policy, ALOHA environments and datasets. Ours are adapted from [ALOHA](https://tonyzhaozh.github.io/aloha) and [Mobile ALOHA](https://mobile-aloha.github.io).
- Thanks to Cheng Chi, Zhenjia Xu and colleagues for open sourcing Diffusion policy, Pusht environment and datasets, as well as UMI datasets. Ours are adapted from [Diffusion Policy](https://diffusion-policy.cs.columbia.edu) and [UMI Gripper](https://umi-gripper.github.io).
- Thanks to Nicklas Hansen, Yunhai Feng and colleagues for open sourcing TDMPC policy, Simxarm environments and datasets. Ours are adapted from [TDMPC](https://github.com/nicklashansen/tdmpc) and [FOWM](https://www.yunhaifeng.com/FOWM).
- Thanks to Antonio Loquercio and Ashish Kumar for their early support.
- Thanks to [Seungjae (Jay) Lee](https://sjlee.cc/), [Mahi Shafiullah](https://mahis.life/) and colleagues for open sourcing [VQ-BeT](https://sjlee.cc/vq-bet/) policy and helping us adapt the codebase to our repository. The policy is adapted from [VQ-BeT repo](https://github.com/jayLEE0301/vq_bet_official).

## Citation

If you want, you can cite this work with:

```bibtex
@misc{cadene2024lerobot,
    author = {Cadene, Remi and Alibert, Simon and Soare, Alexander and Gallouedec, Quentin and Zouitine, Adil and Palma, Steven and Kooijmans, Pepijn and Aractingi, Michel and Shukor, Mustafa and Aubakirova, Dana and Russi, Martino and Capuano, Francesco and Pascal, Caroline and Choghari, Jade and Moss, Jess and Wolf, Thomas},
    title = {LeRobot: State-of-the-art Machine Learning for Real-World Robotics in Pytorch},
    howpublished = "\url{https://github.com/huggingface/lerobot}",
    year = {2024}
}
```
